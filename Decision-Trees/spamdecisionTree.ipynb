{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    datamat=sio.loadmat('./spam_data/spam_data.mat')\n",
    "    X_train=datamat['training_data']\n",
    "    X_test=datamat['test_data']\n",
    "    y_train=datamat['training_labels']\n",
    "    return X_train, X_test, y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train =load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train=y_train.reshape((y_train.shape[1],1))\n",
    "X_y=np.hstack((X_train,y_train))\n",
    "X_y,X_validation =X_y[:-1000,:],X_y[X_train.shape[0]-1000:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Choosing attribute and a threshold for that attribute\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We want to go through each feature, that is the vector of the values of the same attribute for each sample, and then span that feature with different threshold values for its attributes. Then we want to check their information gains and choose the one that gives us the maximum information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Entropy is defined as $H(\\rho)=-\\sum_i^{n_{class}}\\rho_i\\log_{2}(\\rho_i)$\n",
    "\n",
    "\n",
    "The expected entropy for choosing one attribute from the feature vector is defined as $EH(\\rho_i)=\\sum_{i}^{k}\\frac{p_i+ n_i}{n + p}H(\\rho_i)$\n",
    "\n",
    "\n",
    "Here $p_i$ and $n_i$ represent the number of samples of the two classes in each children respectively. $n + p$ is the total number of examples of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The entropy for the root\n",
    "def H(X):\n",
    "    if X.shape[0]==0:\n",
    "        return 0\n",
    "    #Takes the classes \n",
    "    labels=X[:,-1]\n",
    "    examples = float(labels.shape[0])\n",
    "    n=float(np.count_nonzero(labels)) #number of samples that belong to class (Y=1)\n",
    "    if n==examples:\n",
    "        p=0.0\n",
    "    else:\n",
    "        p=float(labels.shape[0]-n) #number of samples that belong to class (Y=0)\n",
    "    p1=n/examples\n",
    "    p2=p/examples\n",
    "    log2 = lambda x: math.log(x)/math.log(2)\n",
    "    return -p1*log2(p1+ 0.0001)-p2*log2(p2+0.0001)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the case for only two classes, we have a binary decision tree, that means that after the feature passes through a node the decision rule of node will divide the samples in two groups. We then just need to worry about two children coming out of each parent every time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will then need to choose a value for our threshold and an attribute. We have multiple features and each feature is going to spand different values. To chose the best threshold using the greedy algorightm we can simply span a set of thresholds for each feature and  choose which  will be the best threshold and attribute  by checking the Information gain of each combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We described the information gain for a certain attribute as follows. $I(\\rho_i)=H(\\rho_i)-EH(\\rho_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The last column of the rows will be its class labels for each feature\n",
    "\n",
    "def split_set(rows,col, value):\n",
    "    Lset=[]\n",
    "    Rset=[]\n",
    "    for row in rows:\n",
    "        if row[col] >= value:\n",
    "            Lset.append(row)\n",
    "        else:\n",
    "            Rset.append(row)\n",
    "    Lset=np.array(Lset)\n",
    "    Rset=np.array(Rset)\n",
    "    return Lset, Rset\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Expectation Entropy function, tells us the weigthed sum  of the two children\n",
    "\n",
    "def EH(Left,Right):\n",
    "    examplesL=float(Left.shape[0]) # examples in the child on the left\n",
    "    examplesR=float(Right.shape[0]) # examples in the child on the right\n",
    "    p=examplesL/(examplesL + examplesR)\n",
    "    return (p)*H(Left)+(1-p)*H(Right)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Function to choose the thresholds\n",
    "'''1. Sort the feature vector\n",
    "   2. Starting from the smallest number compare adjecent rumbers as you ascend to higher values\n",
    "   3. Only for those adjecent numbers that differ calculate the midpoint between them\n",
    "   4. Return the array all the allowed midpoints'''\n",
    "def thresholds(rows,column):\n",
    "    feature=np.sort(np.unique(rows[:,column]))\n",
    "    place =feature[0] #Lessser value\n",
    "    midpoints=[]\n",
    "    for i in range(feature.shape[0]-1):\n",
    "        if place != feature[i+1]:\n",
    "            midpoints.append((feature[i+1]+place)/2)\n",
    "            place=feature[i+1]\n",
    "        else:\n",
    "            place=feature[i+1]\n",
    "    return np.array(midpoints)\n",
    "    \n",
    "def thresholds(rows,column):\n",
    "    return np.unique(rows[:,column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This is a test for our chooser for our decision rule for a node. That is choosing the  best feature and threshold that will\n",
    "#give us the best Information gain\n",
    "def infGain(entropy,expectedE):\n",
    "    return entropy - expectedE\n",
    "\n",
    "def decide_rule(X):\n",
    "    #Returns an array containing the Infomation Gain, \n",
    "    results=[] \n",
    "    #This is the entropy of the parent\n",
    "    entropy=H(X)\n",
    "    Gain1=0.0\n",
    "    thresh=0.0\n",
    "    index=0\n",
    "    for i in range(X.shape[1]-1):\n",
    "        #This for loop iterates through all the features in the set\n",
    "        Thresholds=thresholds(X,i)\n",
    "        for threshold in Thresholds:\n",
    "            #This for loop iterates through the selected thresholds of the ith feature\n",
    "            left, right = split_set(X,i,threshold)\n",
    "            #This is the Information gain \n",
    "            Gain2=infGain(entropy,EH(left,right))\n",
    "            if Gain1 < Gain2:\n",
    "                Gain1=Gain2\n",
    "                thresh=threshold\n",
    "                index=i\n",
    "    return Gain1, thresh, index\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Creating a decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So far we've only been working on chooing the best decision rule of a node of a decision tree. We want to apply this recursively to all the nodes so as to get the least entropy as the last level of children. We want each leaf of the tree to be as pure as possible at the bottom, so the optimum leaf would only contain examples belonging to one single class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So then how do build this decision tree with what we know?\n",
    "Well want to essentially proceed with the same steps as we did before but span this procedure to all the necessary branches needed to come as close as we can to the optimum leaves without overfitting the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We essentially wasn to apply the previous steps in a recurive matter down the decision tree. Let's recap and create a couple guidelines first.\n",
    "\n",
    "A decision tree will take a set of examples and by applying a set of decision rules at the nodes that will split the examples in two groups (for non-binary decision tree it can be more than 2 groups) which will arrive to different nodes where another decision rule will be applied at each node . This process will continue until we get until the leaves which in theory should give us sets of pure examples in each leaf.\n",
    "\n",
    "1.- First we can account for the impurity of the initial set of examples by calculating the entropy as shown above.\n",
    "\n",
    "2.- Choosing the first decision rule will rely on choosing the attribute and threshold value that give the best Infomation gain(the difference between the current entropy and the expected entropy). That is this process will try to divide the whole set in two groups and  get their weighted sum of entropies or expected entropy and choose the decision rule based on which set of attribute and threshold value give the lest expected entropy .\n",
    "\n",
    "3.- Then we know that the purpose of this algorithm is to calculate the information gain for every attribute and choosing the one that gives the highest infomation gain. We will simply do this again and again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then we can write this algorightm recursively following the creteria above. In my case im going to choose to implement this tree as a class for organization purposese.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class decisionNode:\n",
    "    '''This represents the class for the nodes where the decision rule will be inserted. That is the atribute or column\n",
    "    and  the threshold of value selected'''\n",
    "    def __init__(self,col=-1,value=None,results=None,Lb=None,Rb=None):\n",
    "        self.col=col\n",
    "        self.value=value\n",
    "        self.results=results\n",
    "        self.Lb=Lb #left brach\n",
    "        self.Rb=Rb #right branch\n",
    "        \n",
    "    def myclass(self):\n",
    "        if self.results is not None:\n",
    "            counts=np.bincount(self.results.astype(int))\n",
    "            return np.argmax(counts)\n",
    "    \n",
    "def pure(Set):\n",
    "    classes=np.unique(Set[:,Set.shape[1]-1]).shape[0]\n",
    "    if classes==1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#Creating the nodes recursively as stated before\n",
    "#Main things to check is that if the examples at the certain node \n",
    "def decisionTree(Set,max_depth, count):\n",
    "    if type(Set).__name__ != 'ndarray':\n",
    "        raise NameError(\"Oops, not a ndarray. Try again.\")\n",
    "    if Set.shape[0]==0:\n",
    "        #returns one or zero when the there are no examples on the node\n",
    "        print(\"ho\")\n",
    "        tree[count][count+1]=decisionNode(results=np.array([np.random.randint(2)]))\n",
    "        return tree\n",
    "    y_index=Set.shape[1]-1\n",
    "        \n",
    "    tree ={count:{}}\n",
    "    if pure(Set):\n",
    "        tree[count][count+1]=decisionNode(results=Set[:,y_index])\n",
    "        return tree\n",
    "    #Criteria for tree\n",
    "    #if currentGain==None:\n",
    "        #previousGain=0.0\n",
    "    #else:\n",
    "        #previousGain=currentGain\n",
    "    max_count=max_depth\n",
    "    #Decide_rule returns the best highest gain, the threshold, and the column of the feature selected.   \n",
    "    Gain, threshold, column=decide_rule(Set)\n",
    "    #splits the incoming set into branches\n",
    "    leftBranch,rightBranch = split_set(Set,column,threshold)\n",
    "    #Creating a tree for the dictionaries\n",
    "    #Spreading the branches down the tree\n",
    "    if Gain>1e-6 and count<=max_count:\n",
    "        leftBranch=decisionTree(leftBranch,max_depth,count=count+1)\n",
    "        rightBranch=decisionTree(rightBranch,max_depth,count=count+1)\n",
    "        tree[count][count+1]=decisionNode(col=column,value=threshold,Lb=leftBranch,Rb=rightBranch)\n",
    "    else:\n",
    "        tree[count][count+1]=decisionNode(results=Set[:,y_index])\n",
    "        \n",
    "    return tree\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tree=decisionTree(X_y,50,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87392138063279"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_accuracy(tree,X_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Before we can test any new data we must first realize that we have only created a decision tree and we still have figure out how to classify correctly new inconming examples. How are we going to do this well the easiest way to achieve this is by first looking at a single example and figure out  which leaf it will land on. After it may come to your attention that we have already classified training examples before so looking at those already classified examples in each node we can look for the distribution of examples in each leaf. That is if there's only one class of examples in the class then the probability that an example that lands on that leaf being of that certain class is completely 100%. If there exist a probability dirstribution for the classes then we pick the highest probability and we assign the test examples we the class of the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Classifier will essentially lead the test examples down the tree by essentially picking the attribute column and using\n",
    "#the threshold stored in the decision node to apply the same procedure down the tree until we get to the bottom\n",
    "def single_prediction(tree,example,count=1):\n",
    "    \n",
    "    node=tree[count][count+1]\n",
    "  \n",
    "    if node.results is not None:\n",
    "        return node.myclass()\n",
    "    feature_index=node.col\n",
    "    threshold=node.value\n",
    "    if example[feature_index]>=threshold:\n",
    "        return single_prediction(node.Lb,example,count+1)\n",
    "    else:\n",
    "        return single_prediction(node.Rb,example,count+1)\n",
    "\n",
    "\n",
    "def classification_accuracy(tree,testSet):\n",
    "    \"Returns the accuracy of the test\"\n",
    "    label_index=testSet.shape[1]-1\n",
    "    correct_labels=0\n",
    "    for example in testSet:\n",
    "        prediction_label=single_prediction(tree,example)\n",
    "        if prediction_label==int(example[label_index]):\n",
    "            correct_labels+=1\n",
    "    return float(correct_labels)/testSet.shape[0]\n",
    "    \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Making a random subset of data for a tree with a set number of examples\n",
    "def sampleSet(Set,size):\n",
    "    subset=[]\n",
    "    for i in np.arange(size):\n",
    "        row=np.random.randint(Set.shape[0])\n",
    "        subset.append(Set[row,:])\n",
    "    return np.array(subset)\n",
    "# Here we will create a Random forest tree\n",
    "\n",
    "def randomForest(Set,subset_size,max_depth,n_trees):\n",
    "    trees=[]\n",
    "    for i in np.arange(n_trees):\n",
    "        subset=sampleSet(Set,subset_size)\n",
    "        tree=decisionTree(subset,max_depth,1)\n",
    "        trees.append(tree)\n",
    "    return trees\n",
    "#Calculates the prediction of one example of random decision trees utilizing an extension of bagging for Random Forests\n",
    "def prediction_bagging(trees,testExample):\n",
    "    single_predictions=[single_prediction(tree,testExample) for tree in trees]\n",
    "    counts=np.bincount(np.array(single_predictions))\n",
    "    return np.argmax(counts)\n",
    "\n",
    "def ranForest_predictions(trees,testSet):\n",
    "    predictions=[]\n",
    "    for row in np.arange(testSet.shape[0]):\n",
    "        predictions.append(prediction_bagging(trees,testSet[row,:]))\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "\n",
    "def forest_accuracy(trees,Set):\n",
    "    labels=Set[:,-1]\n",
    "    correct_labels=0\n",
    "    predictions=ranForest_predictions(trees,Set)\n",
    "    for i in np.arange(labels.shape[0]):\n",
    "        if predictions[i]==int(labels[i]):\n",
    "            correct_labels+=1\n",
    "    return float(correct_labels)/labels.shape[0]\n",
    "# Prediction for kaggle   \n",
    "def predict_kaggle(trees,Set):\n",
    "    predictions=ranForest_predictions(trees,Set)\n",
    "    with open('kaggle2.csv', 'wb') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile)\n",
    "        spamwriter.writerow(['Id','Category'])\n",
    "        k=1\n",
    "        for i in predictions:\n",
    "            spamwriter.writerow([k, i])\n",
    "            k+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trees=randomForest(X_y,900,16,43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the training accuracy:\n",
      "0.832454458293\n",
      "This is the validation accuracy\n",
      "0.857\n"
     ]
    }
   ],
   "source": [
    "print (\"This is the training accuracy:\")\n",
    "print(forest_accuracy(trees,X_y))\n",
    "\n",
    "print (\"This is the validation accuracy\")\n",
    "print(forest_accuracy(trees,X_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predict_kaggle(trees,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
